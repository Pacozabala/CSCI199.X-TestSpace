{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcwIx6BXvMgt7qoxUz2j3a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pacozabala/CSCI199.X-TestSpace/blob/main/data_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "HUG2lmzwYzoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d asaniczka/reddit-on-israel-palestine-daily-updated\n",
        "!unzip reddit-on-israel-palestine-daily-updated.zip"
      ],
      "metadata": {
        "id": "ChxjG4CjY-xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "63HJpvAqcG6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"reddit_opinion_PSE_ISR.csv\", dtype={10: str})"
      ],
      "metadata": {
        "id": "fmmZGhY-POoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "j_tYJzYp1r-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter between Oct and Dec 2023\n",
        "df['post_created_time'] = pd.to_datetime(df['post_created_time'])\n",
        "\n",
        "start_date = pd.to_datetime('2023-10-01')\n",
        "end_date = pd.to_datetime('2023-12-31')\n",
        "\n",
        "df_dated = df[\n",
        "    (df['post_created_time'] >= start_date) &\n",
        "    (df['post_created_time'] <= end_date)\n",
        "]\n",
        "\n",
        "print(df_dated['post_created_time'].min())\n",
        "print(df_dated['post_created_time'].max())"
      ],
      "metadata": {
        "id": "Zx_qjQr7Ay87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out posts from underrepresented subreddits\n",
        "subreddit_counts = df_dated['subreddit'].value_counts()\n",
        "valid_subreddits = subreddit_counts[subreddit_counts >= 1000].index\n",
        "df_dated = df_dated[df_dated['subreddit'].isin(valid_subreddits)]\n",
        "df_dated.info()"
      ],
      "metadata": {
        "id": "wP-BEQOwQecu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out null values\n",
        "df_dated = df_dated.dropna(subset=['post_self_text'])\n",
        "display(df_dated[['post_self_text']].head())"
      ],
      "metadata": {
        "id": "rNBZVzsIExXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "966d364d"
      },
      "source": [
        "# get a random sample of 1000\n",
        "df_sample = df_dated.sample(n=1000, random_state=42) # using a random state for reproducibility\n",
        "display(df_sample.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning text\n",
        "# remove html tags, user mentions, subreddit references\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. remove HTML tags, CSS styles\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # 2. remove user mentions like \"u/username\"\n",
        "    text = re.sub(r\"u/[A-Za-z0-9_-]+\", \"\", text)\n",
        "\n",
        "    # 3. remove subreddit mentions\"\n",
        "    text = re.sub(r\"r/[A-Za-z0-9_-]+\", \"\", text)\n",
        "\n",
        "    # 4. remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "\n",
        "    # 5. remove whitespace and line breaks\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # 6. lowercase text\n",
        "    text = text.lower()\n",
        "\n",
        "    # 7. remove punctuation, but keep periods, question marks, and exclamation points\n",
        "    text = re.sub(r\"[^\\w\\s.?!]\", \"\", text)\n",
        "\n",
        "    return text\n",
        "df_cleaned = df_sample.copy()\n",
        "df_cleaned['cleaned_text'] = df_sample['post_self_text'].apply(clean_text)\n",
        "display(df_cleaned[['post_self_text', 'cleaned_text']].head(10))"
      ],
      "metadata": {
        "id": "O9ImUb7MFpkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c49b802"
      },
      "source": [
        "# get needed columns\n",
        "df_column = df_cleaned[['cleaned_text']]\n",
        "df_column.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a348087"
      },
      "source": [
        "# eliminate duplicates\n",
        "df_unique = df_column.drop_duplicates(subset=['cleaned_text'])\n",
        "display(df_unique.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "001db774"
      },
      "source": [
        "# sentence segmentation and punctuation removal\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "\n",
        "# Download all 'punkt' related resources if they haven't already been downloaded\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading 'punkt' resources: {e}\")\n",
        "\n",
        "\n",
        "def segment_and_clean_sentences(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    sentences = sent_tokenize(text)\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Remove all punctuation from the sentence\n",
        "        sentence_no_punct = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "        cleaned_sentences.append(sentence_no_punct)\n",
        "    return cleaned_sentences\n",
        "\n",
        "# Create a new list to store the individual sentences\n",
        "sentences_list = []\n",
        "for index, row in df_unique.iterrows():\n",
        "    cleaned_sentences = segment_and_clean_sentences(row['cleaned_text'])\n",
        "    for sentence in cleaned_sentences:\n",
        "        sentences_list.append({'sentence': sentence})\n",
        "\n",
        "# Create a new DataFrame from the list of sentences\n",
        "df_sentences = pd.DataFrame(sentences_list)\n",
        "display(df_sentences.head())\n",
        "df_sentences.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mock data generation\n",
        "mft_categories = [\"care/harm\", \"fairness/cheating\", \"loyalty/betrayal\", \"authority/subversion\", \"purity/degradation\", \"none\"]\n",
        "polarities = [\"positive\", \"negative\", \"neutral\"]\n",
        "\n",
        "data = []\n",
        "for i, row in df_sentences.iterrows():\n",
        "    sentence = row['sentence']\n",
        "\n",
        "    # Randomly assign category and polarity\n",
        "    category = random.choice(mft_categories)\n",
        "    polarity = random.choice(polarities)\n",
        "    category_polarity = f\"{category} {polarity}\"\n",
        "\n",
        "    # 70% explicit targets, 30% implicit\n",
        "    entailed = \"yes\" if random.random() > 0.3 else \"no\"\n",
        "\n",
        "    if entailed == \"yes\":\n",
        "        words = sentence.split()\n",
        "        if len(words) > 3:\n",
        "            start = random.randint(0, len(words) - 2)\n",
        "            end = start + 1\n",
        "            target = words[start]\n",
        "        else:\n",
        "            start, end, target = 0, 0, \"\"\n",
        "    else:\n",
        "        start, end, target = 0, 0, \"\"\n",
        "\n",
        "    sentence_id = f\"{1000000 + i}:{0}\"\n",
        "    data.append({\n",
        "        \"sentence_id\": sentence_id,\n",
        "        \"sentence\": sentence,\n",
        "        \"target\": target,\n",
        "        \"category\": category,\n",
        "        \"polarity\": polarity,\n",
        "        \"category_polarity\": category_polarity,\n",
        "        \"entailed\": entailed,\n",
        "        \"start\": start,\n",
        "        \"end\": end\n",
        "    })\n",
        "\n",
        "df_mock = pd.DataFrame(data)\n",
        "df_mock.head()"
      ],
      "metadata": {
        "id": "B5QSwu-Z1AqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4b15eb5"
      },
      "source": [
        "# shuffle dataframe\n",
        "df_shuffled = df_mock.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# calculate split point\n",
        "split_point = int(len(df_shuffled) * 0.8)\n",
        "\n",
        "# split into training and testing sets\n",
        "df_train = df_shuffled[:split_point]\n",
        "df_test = df_shuffled[split_point:]\n",
        "\n",
        "# save to CSV files\n",
        "df_train.to_csv('df_mock_train.csv', index=False)\n",
        "df_test.to_csv('df_mock_test.csv', index=False)\n",
        "\n",
        "print(\"Training set shape:\", df_train.shape)\n",
        "print(\"Test set shape:\", df_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2947df9a"
      },
      "source": [
        "# Save df_train to a TSV file\n",
        "df_train.to_csv('df_mock_train.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Save df_test to a TSV file\n",
        "df_test.to_csv('df_mock_test.tsv', sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3af1b76"
      },
      "source": [
        "!git clone https://github.com/sysulic/TAS-BERT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "970b1ae2"
      },
      "source": [
        "!pip install pytorch-crf\n",
        "import torchcrf\n",
        "print(\"torchcrf imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fcedf52"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip -o uncased_L-12_H-768_A-12.zip -d TAS-BERT/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# command to create BERT-pytorch-model\n",
        "!python TAS-BERT/convert_tf_checkpoint_to_pytorch.py \\\n",
        "--tf_checkpoint_path TAS-BERT/uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
        "--bert_config_file TAS-BERT/uncased_L-12_H-768_A-12/bert_config.json \\\n",
        "--pytorch_dump_path TAS-BERT/uncased_L-12_H-768_A-12/pytorch_model.bin"
      ],
      "metadata": {
        "id": "-D-0WXqflihS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data pre-prep command\n",
        "!cd TAS-BERT && cd data && python data_preprocessing_for_TAS.py --dataset semeval2015 && python data_preprocessing_for_TAS.py --dataset semeval2016"
      ],
      "metadata": {
        "id": "TROrG5c9SRmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# command to train + test model\n",
        "!cd TAS-BERT && CUDA_VISIBLE_DEVICES=0 python TAS_BERT_joint.py \\\n",
        "--data_dir data/semeval2016/three_joint/BIO/ \\\n",
        "--output_dir results/semeval2016/three_joint/BIO/my_result \\\n",
        "--vocab_file uncased_L-12_H-768_A-12/vocab.txt \\\n",
        "--bert_config_file uncased_L-12_H-768_A-12/bert_config.json \\\n",
        "--init_checkpoint uncased_L-12_H-768_A-12/pytorch_model.bin \\\n",
        "--tokenize_method word_split \\\n",
        "--use_crf \\\n",
        "--eval_test \\\n",
        "--do_lower_case \\\n",
        "--max_seq_length 128 \\\n",
        "--train_batch_size 24 \\\n",
        "--eval_batch_size 8 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--num_train_epochs 30.0"
      ],
      "metadata": {
        "id": "9RU0FQyGjJW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSUqEUOKjKR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}